{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "CartPole_v1_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "6c49mUy-WRPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cac2484-736b-409c-e1b0-e1fd753a646d"
      },
      "source": [
        "# install necessary packages for rendering openAI gym environment\n",
        "!apt-get update -qq\n",
        "!pip install --upgrade pip --quiet\n",
        "\n",
        "!apt-get install python-opengl swig cmake libopenmpi-dev zlib1g-dev xvfb x11-utils ffmpeg -qq\n",
        "!pip install stable-baselines[mpi] box2d box2d-kengz pyvirtualdisplay pyglet==1.3.1 --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5MB 12.4MB/s \n",
            "\u001b[?25hSelecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 145480 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package python-opengl.\n",
            "Preparing to unpack .../1-python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../2-swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../3-swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../4-x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../5-xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 15.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 56.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 425 kB 64.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 240 kB 51.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 71.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 53.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 42.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 62.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 54.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 54.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 54.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 51.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 68.6 MB/s \n",
            "\u001b[?25h  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mpi4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.11.0 requires cloudpickle==1.3, but you have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNWjyNCtWRPf",
        "outputId": "4acff3eb-a07d-42e4-a9fa-0fcaac034da4"
      },
      "source": [
        "# start a virtual display\n",
        "import os\n",
        "import pyvirtualdisplay\n",
        "\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1024, 768))\n",
        "display.start()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f36dc851b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIRuRdySWRPi",
        "outputId": "6f69e3ad-a469-4acc-d3fd-6f350f38412e"
      },
      "source": [
        "# we need GLX for rendering the episodes, so check if GLX is available\n",
        "glxinfo = !xdpyinfo | grep GLX\n",
        "\n",
        "for line in glxinfo:\n",
        "    if line.strip() == 'GLX':\n",
        "        print('GLX is available')\n",
        "        break\n",
        "else:\n",
        "    print('GLX is unavailable')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GLX is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "t_Rrrso1WRPj"
      },
      "source": [
        "# necessary imports\n",
        "\n",
        "import time\n",
        "import itertools\n",
        "from collections import namedtuple\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as dist"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6KUGOTuWRPl",
        "outputId": "c4223d21-05a2-448b-fb55-6173545b847a"
      },
      "source": [
        "# use cuda if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    # allocated, cached = torch.cuda.memory_allocated(0), torch.cuda.memory_cached(0)    # memory_cached is deprecated\n",
        "    allocated, cached = torch.cuda.memory_allocated(0), torch.cuda.memory_reserved(0)\n",
        "    allocated, cached = round(allocated / 1024**3, 1), round(cached / 1024**3, 1)\n",
        "\n",
        "    print(f'Device name: {device_name}')\n",
        "    print(f'Memory Usage: Allocated {allocated}GB, Cached {cached}GB')\n",
        "else:\n",
        "    print('CUDA is unavailable')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Device name: Tesla T4\n",
            "Memory Usage: Allocated 0.0GB, Cached 0.0GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "h29OsqvuWRPn"
      },
      "source": [
        "# simple FNN to learn mapping from state to action\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, observation_size, no_of_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(observation_size, 128)\n",
        "        self.fc2 = nn.Linear(128, no_of_actions)\n",
        "        self.dropout = nn.Dropout(p=0.6)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.dropout(self.fc1(x)))\n",
        "        return F.softmax(self.fc2(x), dim=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7bXN0pJyWRPo"
      },
      "source": [
        "# some hyperparameters\n",
        "lr = 0.01\n",
        "gamma = 0.99\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "# misc\n",
        "seed, log_interval, record_interval = 2, 10, 50"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wKR5p3reWRPp"
      },
      "source": [
        "# prepare gym environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env_wrapper = gym.wrappers.Monitor(env, directory=\"CartPole_Q_learning\", force=True, video_callable=lambda episode_idx: not bool(episode_idx % record_interval))\n",
        "\n",
        "# get observation space and number of actions of the environment\n",
        "observation_size, no_of_actions = env.observation_space.shape[0], env.action_space.n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCsrvuZG3gNn",
        "outputId": "cbee5dbe-72d4-457b-cfe9-6c8fda7a89f0"
      },
      "source": [
        "# set seed to avoid randomness\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f36dda7de88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qDZVfqOzWRPr"
      },
      "source": [
        "net = Net(observation_size, no_of_actions).to(device=device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=lr)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA-ZChqc_w7r"
      },
      "source": [
        "def get_action(state, saved_log_probs):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    probs = net(state)\n",
        "    m = dist.Categorical(probs)\n",
        "    action = m.sample()\n",
        "    saved_log_probs.append(m.log_prob(action))\n",
        "    return action.item()\n",
        "\n",
        "def run_episode(env, rewards, saved_log_probs, max_steps=10000):\n",
        "    total_reward = 0\n",
        "    state = env.reset()\n",
        "    for timestep in range(max_steps):\n",
        "        action = get_action(state, saved_log_probs)\n",
        "        state, reward, is_done, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        total_reward += reward\n",
        "        if is_done:\n",
        "            break\n",
        "    return total_reward, timestep+1\n",
        "\n",
        "def learn(rewards, saved_log_probs):\n",
        "    R, returns, policy_loss = 0, [], []\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        returns.append(R)\n",
        "    returns = torch.tensor(returns[::-1])\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    for log_prob, R in zip(saved_log_probs, returns):\n",
        "        policy_loss.append(-log_prob * R)\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def main():\n",
        "    running_reward = 10\n",
        "    for episode_idx in itertools.count(1):\n",
        "        rewards, saved_log_probs = [], []\n",
        "        reward, timesteps = run_episode(env_wrapper, rewards, saved_log_probs)\n",
        "        learn(rewards, saved_log_probs)\n",
        "        running_reward = 0.05 * reward + (1 - 0.05) * running_reward\n",
        "        if episode_idx % log_interval == 0:\n",
        "            print(f'{episode_idx:4d}: reward={reward:4.2f}  running_reward={running_reward:4.2f}')\n",
        "        if running_reward > env.spec.reward_threshold:\n",
        "            print(f'reward threshold reached!')\n",
        "            break"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl11p-0yTdiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2aab9f-3693-445b-d952-da9791ebb5bd"
      },
      "source": [
        "start_timer = time.time()\n",
        "main()\n",
        "print(f'Total time taken: {time.time() - start_timer}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  10: reward=15.00  running_reward=15.00\n",
            "  20: reward=49.00  running_reward=32.84\n",
            "  30: reward=65.00  running_reward=40.67\n",
            "  40: reward=43.00  running_reward=50.56\n",
            "  50: reward=87.00  running_reward=51.61\n",
            "  60: reward=79.00  running_reward=59.28\n",
            "  70: reward=157.00  running_reward=87.11\n",
            "  80: reward=161.00  running_reward=112.21\n",
            "  90: reward=244.00  running_reward=131.09\n",
            " 100: reward=199.00  running_reward=154.18\n",
            " 110: reward=201.00  running_reward=173.51\n",
            " 120: reward=178.00  running_reward=201.72\n",
            " 130: reward=182.00  running_reward=187.51\n",
            " 140: reward=138.00  running_reward=164.24\n",
            " 150: reward=329.00  running_reward=171.90\n",
            " 160: reward=108.00  running_reward=256.15\n",
            " 170: reward=324.00  running_reward=325.75\n",
            " 180: reward=186.00  running_reward=339.78\n",
            " 190: reward=158.00  running_reward=287.15\n",
            " 200: reward=121.00  running_reward=224.82\n",
            " 210: reward=72.00  running_reward=176.36\n",
            " 220: reward=35.00  running_reward=144.69\n",
            " 230: reward=110.00  running_reward=131.25\n",
            " 240: reward=105.00  running_reward=113.50\n",
            " 250: reward=119.00  running_reward=107.19\n",
            " 260: reward=158.00  running_reward=124.78\n",
            " 270: reward=197.00  running_reward=143.03\n",
            " 280: reward=135.00  running_reward=144.84\n",
            " 290: reward=122.00  running_reward=140.21\n",
            " 300: reward=122.00  running_reward=129.36\n",
            " 310: reward=118.00  running_reward=132.73\n",
            " 320: reward=169.00  running_reward=137.49\n",
            " 330: reward=197.00  running_reward=151.92\n",
            " 340: reward=164.00  running_reward=165.34\n",
            " 350: reward=290.00  running_reward=183.89\n",
            " 360: reward=465.00  running_reward=262.10\n",
            " 370: reward=483.00  running_reward=308.53\n",
            " 380: reward=85.00  running_reward=258.57\n",
            " 390: reward=209.00  running_reward=214.47\n",
            " 400: reward=500.00  running_reward=320.58\n",
            " 410: reward=391.00  running_reward=384.37\n",
            " 420: reward=137.00  running_reward=315.96\n",
            " 430: reward=37.00  running_reward=225.39\n",
            " 440: reward=35.00  running_reward=160.06\n",
            " 450: reward=55.00  running_reward=117.60\n",
            " 460: reward=70.00  running_reward=96.32\n",
            " 470: reward=10.00  running_reward=77.62\n",
            " 480: reward=52.00  running_reward=60.61\n",
            " 490: reward=30.00  running_reward=52.42\n",
            " 500: reward=39.00  running_reward=48.19\n",
            " 510: reward=58.00  running_reward=47.26\n",
            " 520: reward=64.00  running_reward=52.51\n",
            " 530: reward=70.00  running_reward=57.33\n",
            " 540: reward=101.00  running_reward=66.73\n",
            " 550: reward=106.00  running_reward=83.55\n",
            " 560: reward=112.00  running_reward=89.29\n",
            " 570: reward=46.00  running_reward=86.59\n",
            " 580: reward=63.00  running_reward=80.58\n",
            " 590: reward=92.00  running_reward=77.32\n",
            " 600: reward=82.00  running_reward=77.98\n",
            " 610: reward=72.00  running_reward=78.06\n",
            " 620: reward=80.00  running_reward=80.32\n",
            " 630: reward=93.00  running_reward=80.15\n",
            " 640: reward=82.00  running_reward=79.88\n",
            " 650: reward=62.00  running_reward=75.63\n",
            " 660: reward=54.00  running_reward=72.76\n",
            " 670: reward=70.00  running_reward=70.25\n",
            " 680: reward=84.00  running_reward=71.03\n",
            " 690: reward=81.00  running_reward=74.84\n",
            " 700: reward=94.00  running_reward=82.17\n",
            " 710: reward=142.00  running_reward=91.50\n",
            " 720: reward=106.00  running_reward=101.91\n",
            " 730: reward=127.00  running_reward=112.94\n",
            " 740: reward=324.00  running_reward=165.43\n",
            " 750: reward=500.00  running_reward=265.41\n",
            " 760: reward=325.00  running_reward=319.07\n",
            " 770: reward=292.00  running_reward=308.09\n",
            " 780: reward=500.00  running_reward=327.64\n",
            " 790: reward=322.00  running_reward=372.04\n",
            " 800: reward=500.00  running_reward=411.99\n",
            " 810: reward=255.00  running_reward=401.15\n",
            " 820: reward=242.00  running_reward=340.19\n",
            " 830: reward=273.00  running_reward=314.33\n",
            " 840: reward=301.00  running_reward=334.86\n",
            " 850: reward=191.00  running_reward=303.91\n",
            " 860: reward=208.00  running_reward=263.70\n",
            " 870: reward=168.00  running_reward=243.19\n",
            " 880: reward=214.00  running_reward=231.90\n",
            " 890: reward=244.00  running_reward=251.82\n",
            " 900: reward=500.00  running_reward=321.83\n",
            " 910: reward=500.00  running_reward=393.32\n",
            " 920: reward=500.00  running_reward=436.13\n",
            " 930: reward=500.00  running_reward=461.76\n",
            " 940: reward=500.00  running_reward=475.02\n",
            "reward threshold reached!\n",
            "Total time taken: 244.90627264976501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h59AkVb8VbQc"
      },
      "source": [
        "# finally, close gym environment\n",
        "env_wrapper.close()\n",
        "env.close()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9xyKmynVdKF"
      },
      "source": [
        "# save the model parameters\n",
        "torch.save(net, 'CartPole_Q_learning.pth')"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}