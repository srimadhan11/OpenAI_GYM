{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Copy of LunarLander.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "6c49mUy-WRPb"
      },
      "source": [
        "# install necessary packages for rendering openAI gym environment\n",
        "!apt-get update -qq\n",
        "!pip install --upgrade pip --quiet\n",
        "\n",
        "!apt-get install python-opengl swig cmake libopenmpi-dev zlib1g-dev xvfb x11-utils ffmpeg -qq\n",
        "!pip install stable-baselines[mpi] box2d box2d-kengz pyvirtualdisplay pyglet==1.3.1 --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNWjyNCtWRPf",
        "outputId": "3aa5e6b7-1cbf-423b-d07d-d581d0dfc471"
      },
      "source": [
        "# start a virtual display\n",
        "import os\n",
        "import pyvirtualdisplay\n",
        "\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1024, 768))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f480988af60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIRuRdySWRPi",
        "outputId": "eb85e765-a035-49cb-e085-9c9adc3b001e"
      },
      "source": [
        "# we need GLX for rendering the episodes, so check if GLX is available\n",
        "glxinfo = !xdpyinfo | grep GLX\n",
        "\n",
        "for line in glxinfo:\n",
        "    if line.strip() == 'GLX':\n",
        "        print('GLX is available')\n",
        "        break\n",
        "else:\n",
        "    print('GLX is unavailable')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GLX is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "t_Rrrso1WRPj"
      },
      "source": [
        "# necessary imports\n",
        "\n",
        "import time\n",
        "from collections import namedtuple\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6KUGOTuWRPl",
        "outputId": "dbcb9e28-e4ff-4d8d-c116-8ce7f8522ad8"
      },
      "source": [
        "# use cuda if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    # allocated, cached = torch.cuda.memory_allocated(0), torch.cuda.memory_cached(0)    # memory_cached is deprecated\n",
        "    allocated, cached = torch.cuda.memory_allocated(0), torch.cuda.memory_reserved(0)\n",
        "    allocated, cached = round(allocated / 1024**3, 1), round(cached / 1024**3, 1)\n",
        "\n",
        "    print(f'Device name: {device_name}')\n",
        "    print(f'Memory Usage: Allocated {allocated}GB, Cached {cached}GB')\n",
        "else:\n",
        "    print('CUDA is unavailable')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "CUDA is unavailable\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoHhhzsWEz4M"
      },
      "source": [
        "# simple FNN to learn mapping from state to action\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, observation_size, no_of_actions):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(observation_size, 128)\n",
        "    self.fc2 = nn.Linear(128, no_of_actions)\n",
        "    self.dropout = nn.Dropout(p=0.6)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.dropout(self.fc1(x)))\n",
        "    return F.softmax(self.fc2(x), dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7bXN0pJyWRPo"
      },
      "source": [
        "# some hyperparameters\n",
        "lr = 0.01\n",
        "steps = 500\n",
        "no_of_episodes, generations = 1000, 500\n",
        "percentile, expected_reward_mean = 70, 200\n",
        "\n",
        "# misc\n",
        "seed = 2\n",
        "required_reward_threshold = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wKR5p3reWRPp"
      },
      "source": [
        "# prepare gym environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env_wrapper = gym.wrappers.Monitor(env, directory=\"CartPole_simple\", force=True, video_callable=lambda episode_idx: True)\n",
        "\n",
        "# get observation space and number of actions of the environment\n",
        "observation_size, no_of_actions = env.observation_space.shape[0], env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY2HYVPkQ2Zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2d5f7f-c17a-4889-bf28-3ca7d3c6dd27"
      },
      "source": [
        "# set seed to avoid randomness\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f480aabce88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qDZVfqOzWRPr"
      },
      "source": [
        "net = Net(observation_size, no_of_actions).to(device=device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WdE1GXuOWRPs"
      },
      "source": [
        "def generate_episode(env, net, max_steps=1000):\n",
        "    states, actions, episode_reward = [], [], 0\n",
        "    state = env.reset()\n",
        "    for step in range(max_steps):\n",
        "        tensor_state = torch.FloatTensor([state]).to(device=device)\n",
        "        act_probs = net(tensor_state)\n",
        "        act_probs = act_probs.data.cpu().numpy()[0]\n",
        "        action = np.random.choice(act_probs.shape[0], p=act_probs)\n",
        "        next_state, reward, is_done, info = env.step(action)\n",
        "        \n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        episode_reward += reward\n",
        "        \n",
        "        state = next_state\n",
        "        if is_done:\n",
        "            return True, (states, actions, episode_reward)\n",
        "    return False, None\n",
        "\n",
        "\n",
        "def generate_episodes(env, net, episode_size, max_steps=1000):\n",
        "    e_states, e_actions, e_rewards = [], [], []\n",
        "    for episode in range(episode_size):\n",
        "        is_done, episode_data = generate_episode(env, net, max_steps)\n",
        "        if is_done:\n",
        "            states, actions, episode_reward = episode_data\n",
        "            e_states.append(states)\n",
        "            e_actions.append(actions)\n",
        "            e_rewards.append(episode_reward)\n",
        "    return e_states, e_actions, e_rewards\n",
        "\n",
        "\n",
        "def filter_episodes(states, actions, rewards, percentile=70):\n",
        "    filtered_states, filtered_actions = [], []\n",
        "    reward_threshold = np.percentile(rewards, percentile)\n",
        "    \n",
        "    # add entire episode to (filtered_states, filtered_actions),\n",
        "    # if its reward is higher than reward_threshold\n",
        "    for idx,reward in enumerate(rewards):\n",
        "        if reward > reward_threshold:\n",
        "            filtered_states.append(np.array(states[idx], dtype=np.float32))\n",
        "            filtered_actions.append(np.array(actions[idx], dtype=np.long))\n",
        "    \n",
        "    return np.concatenate(filtered_states), np.concatenate(filtered_actions), reward_threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lPDqPNTUWRPv"
      },
      "source": [
        "def train_agent():\n",
        "    for gen in range(generations):\n",
        "        start_timer = time.time()\n",
        "        if not gen % record_interval: generate_episode(env_wrapper, net, max_steps=steps)\n",
        "        states, actions, rewards = generate_episodes(env, net, no_of_episodes, max_steps=steps)\n",
        "        filtered_states, filtered_actions, reward_threshold = filter_episodes(states, actions, rewards, percentile)\n",
        "        \n",
        "        tensor_states = torch.from_numpy(filtered_states).to(device=device)\n",
        "        tensor_actions = torch.from_numpy(filtered_actions).to(device=device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        action_probs = net(tensor_states)\n",
        "        loss = criterion(action_probs, tensor_actions)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        reward_mean = np.mean(rewards)\n",
        "        time_per_gen = round(time.time() - start_timer)\n",
        "        print('{:4d}[{:3d}sec]: loss={:.3f}, reward_mean={:.1f}, reward_threshold={:.1f}'.format(\n",
        "            gen, time_per_gen, loss.item(), reward_mean, reward_threshold)\n",
        "        )\n",
        "        \n",
        "        if reward_mean > expected_reward_mean:\n",
        "            print('expected reward mean has been reached')\n",
        "            return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXijmo-neiZ9",
        "outputId": "1b3d9690-47c2-4b07-ddcc-e8a4be8bb7fe"
      },
      "source": [
        "start_timer = time.time()\n",
        "record_interval = 2\n",
        "train_agent()\n",
        "print(f'Total time taken: {time.time() - start_timer}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0[  8sec]: loss=0.695, reward_mean=22.1, reward_threshold=25.0\n",
            "   1[  8sec]: loss=0.686, reward_mean=27.7, reward_threshold=31.0\n",
            "   2[ 11sec]: loss=0.673, reward_mean=34.4, reward_threshold=39.3\n",
            "   3[ 13sec]: loss=0.660, reward_mean=42.9, reward_threshold=51.0\n",
            "   4[ 16sec]: loss=0.651, reward_mean=49.9, reward_threshold=58.0\n",
            "   5[ 17sec]: loss=0.640, reward_mean=55.0, reward_threshold=63.0\n",
            "   6[ 20sec]: loss=0.630, reward_mean=61.2, reward_threshold=69.0\n",
            "   7[ 19sec]: loss=0.620, reward_mean=63.6, reward_threshold=73.0\n",
            "   8[ 22sec]: loss=0.610, reward_mean=66.9, reward_threshold=74.0\n",
            "   9[ 20sec]: loss=0.600, reward_mean=68.1, reward_threshold=75.0\n",
            "  10[ 23sec]: loss=0.590, reward_mean=69.8, reward_threshold=76.0\n",
            "  11[ 23sec]: loss=0.582, reward_mean=70.6, reward_threshold=79.0\n",
            "  12[ 22sec]: loss=0.572, reward_mean=72.0, reward_threshold=78.0\n",
            "  13[ 22sec]: loss=0.561, reward_mean=74.8, reward_threshold=81.0\n",
            "  14[ 23sec]: loss=0.553, reward_mean=73.0, reward_threshold=80.0\n",
            "  15[ 23sec]: loss=0.542, reward_mean=75.0, reward_threshold=82.0\n",
            "  16[ 24sec]: loss=0.534, reward_mean=77.0, reward_threshold=84.3\n",
            "  17[ 23sec]: loss=0.523, reward_mean=75.4, reward_threshold=83.0\n",
            "  18[ 24sec]: loss=0.513, reward_mean=75.3, reward_threshold=83.0\n",
            "  19[ 23sec]: loss=0.503, reward_mean=75.7, reward_threshold=83.0\n",
            "  20[ 24sec]: loss=0.491, reward_mean=78.1, reward_threshold=86.0\n",
            "  21[ 23sec]: loss=0.483, reward_mean=75.0, reward_threshold=81.3\n",
            "  22[ 23sec]: loss=0.473, reward_mean=74.1, reward_threshold=81.0\n",
            "  23[ 22sec]: loss=0.465, reward_mean=72.0, reward_threshold=79.3\n",
            "  24[ 23sec]: loss=0.454, reward_mean=72.3, reward_threshold=80.0\n",
            "  25[ 21sec]: loss=0.447, reward_mean=69.5, reward_threshold=77.3\n",
            "  26[ 21sec]: loss=0.441, reward_mean=69.1, reward_threshold=76.0\n",
            "  27[ 21sec]: loss=0.432, reward_mean=69.0, reward_threshold=76.0\n",
            "  28[ 21sec]: loss=0.426, reward_mean=67.0, reward_threshold=74.0\n",
            "  29[ 20sec]: loss=0.422, reward_mean=67.5, reward_threshold=73.0\n",
            "  30[ 21sec]: loss=0.413, reward_mean=67.8, reward_threshold=72.0\n",
            "  31[ 20sec]: loss=0.412, reward_mean=67.9, reward_threshold=72.0\n",
            "  32[ 21sec]: loss=0.404, reward_mean=67.6, reward_threshold=73.0\n",
            "  33[ 20sec]: loss=0.400, reward_mean=67.3, reward_threshold=72.0\n",
            "  34[ 22sec]: loss=0.399, reward_mean=68.9, reward_threshold=75.0\n",
            "  35[ 21sec]: loss=0.396, reward_mean=69.1, reward_threshold=75.0\n",
            "  36[ 22sec]: loss=0.393, reward_mean=68.8, reward_threshold=76.0\n",
            "  37[ 21sec]: loss=0.390, reward_mean=69.5, reward_threshold=76.0\n",
            "  38[ 22sec]: loss=0.391, reward_mean=70.4, reward_threshold=78.0\n",
            "  39[ 22sec]: loss=0.386, reward_mean=73.5, reward_threshold=82.0\n",
            "  40[ 24sec]: loss=0.387, reward_mean=75.2, reward_threshold=83.0\n",
            "  41[ 23sec]: loss=0.389, reward_mean=77.0, reward_threshold=84.0\n",
            "  42[ 26sec]: loss=0.393, reward_mean=83.6, reward_threshold=89.0\n",
            "  43[ 26sec]: loss=0.393, reward_mean=88.2, reward_threshold=93.0\n",
            "  44[ 30sec]: loss=0.399, reward_mean=97.2, reward_threshold=100.0\n",
            "  45[ 31sec]: loss=0.401, reward_mean=103.7, reward_threshold=107.3\n",
            "  46[ 34sec]: loss=0.400, reward_mean=109.5, reward_threshold=119.0\n",
            "  47[ 33sec]: loss=0.397, reward_mean=108.1, reward_threshold=115.0\n",
            "  48[ 32sec]: loss=0.395, reward_mean=103.4, reward_threshold=109.0\n",
            "  49[ 32sec]: loss=0.393, reward_mean=106.0, reward_threshold=113.3\n",
            "  50[ 32sec]: loss=0.393, reward_mean=104.2, reward_threshold=107.0\n",
            "  51[ 33sec]: loss=0.394, reward_mean=110.2, reward_threshold=113.0\n",
            "  52[ 34sec]: loss=0.390, reward_mean=110.7, reward_threshold=115.0\n",
            "  53[ 34sec]: loss=0.390, reward_mean=113.8, reward_threshold=122.0\n",
            "  54[ 36sec]: loss=0.391, reward_mean=116.8, reward_threshold=123.0\n",
            "  55[ 34sec]: loss=0.386, reward_mean=114.9, reward_threshold=123.0\n",
            "  56[ 35sec]: loss=0.386, reward_mean=112.7, reward_threshold=119.0\n",
            "  57[ 34sec]: loss=0.386, reward_mean=116.0, reward_threshold=126.0\n",
            "  58[ 36sec]: loss=0.386, reward_mean=113.9, reward_threshold=118.0\n",
            "  59[ 34sec]: loss=0.384, reward_mean=114.9, reward_threshold=119.0\n",
            "  60[ 37sec]: loss=0.385, reward_mean=118.9, reward_threshold=125.0\n",
            "  61[ 36sec]: loss=0.385, reward_mean=119.5, reward_threshold=129.0\n",
            "  62[ 37sec]: loss=0.383, reward_mean=121.2, reward_threshold=130.3\n",
            "  63[ 36sec]: loss=0.383, reward_mean=120.8, reward_threshold=127.0\n",
            "  64[ 38sec]: loss=0.383, reward_mean=125.4, reward_threshold=135.0\n",
            "  65[ 40sec]: loss=0.382, reward_mean=131.8, reward_threshold=141.0\n",
            "  66[ 39sec]: loss=0.379, reward_mean=126.7, reward_threshold=138.0\n",
            "  67[ 40sec]: loss=0.381, reward_mean=132.5, reward_threshold=151.0\n",
            "  68[ 39sec]: loss=0.379, reward_mean=127.1, reward_threshold=139.0\n",
            "  69[ 37sec]: loss=0.377, reward_mean=126.3, reward_threshold=139.3\n",
            "  70[ 42sec]: loss=0.379, reward_mean=136.3, reward_threshold=154.3\n",
            "  71[ 40sec]: loss=0.379, reward_mean=134.1, reward_threshold=145.0\n",
            "  72[ 41sec]: loss=0.375, reward_mean=134.5, reward_threshold=150.0\n",
            "  73[ 40sec]: loss=0.373, reward_mean=133.2, reward_threshold=147.0\n",
            "  74[ 42sec]: loss=0.378, reward_mean=135.9, reward_threshold=150.3\n",
            "  75[ 42sec]: loss=0.375, reward_mean=142.0, reward_threshold=152.0\n",
            "  76[ 43sec]: loss=0.374, reward_mean=138.9, reward_threshold=156.0\n",
            "  77[ 42sec]: loss=0.373, reward_mean=140.5, reward_threshold=157.3\n",
            "  78[ 45sec]: loss=0.375, reward_mean=147.1, reward_threshold=165.0\n",
            "  79[ 43sec]: loss=0.374, reward_mean=145.5, reward_threshold=165.0\n",
            "  80[ 45sec]: loss=0.372, reward_mean=143.9, reward_threshold=155.0\n",
            "  81[ 43sec]: loss=0.371, reward_mean=145.1, reward_threshold=157.0\n",
            "  82[ 46sec]: loss=0.375, reward_mean=151.5, reward_threshold=173.0\n",
            "  83[ 46sec]: loss=0.372, reward_mean=152.5, reward_threshold=172.0\n",
            "  84[ 48sec]: loss=0.373, reward_mean=155.9, reward_threshold=172.0\n",
            "  85[ 45sec]: loss=0.373, reward_mean=150.8, reward_threshold=163.3\n",
            "  86[ 49sec]: loss=0.372, reward_mean=158.6, reward_threshold=181.0\n",
            "  87[ 47sec]: loss=0.372, reward_mean=157.8, reward_threshold=178.3\n",
            "  88[ 47sec]: loss=0.371, reward_mean=152.0, reward_threshold=166.0\n",
            "  89[ 48sec]: loss=0.374, reward_mean=161.3, reward_threshold=186.0\n",
            "  90[ 50sec]: loss=0.372, reward_mean=164.0, reward_threshold=181.0\n",
            "  91[ 49sec]: loss=0.373, reward_mean=165.1, reward_threshold=189.0\n",
            "  92[ 52sec]: loss=0.372, reward_mean=170.3, reward_threshold=199.0\n",
            "  93[ 50sec]: loss=0.371, reward_mean=168.6, reward_threshold=191.0\n",
            "  94[ 53sec]: loss=0.370, reward_mean=169.0, reward_threshold=193.3\n",
            "  95[ 51sec]: loss=0.369, reward_mean=170.8, reward_threshold=190.0\n",
            "  96[ 55sec]: loss=0.370, reward_mean=178.1, reward_threshold=212.0\n",
            "  97[ 53sec]: loss=0.372, reward_mean=179.3, reward_threshold=202.0\n",
            "  98[ 58sec]: loss=0.370, reward_mean=181.7, reward_threshold=215.3\n",
            "  99[ 55sec]: loss=0.370, reward_mean=185.8, reward_threshold=216.0\n",
            " 100[ 63sec]: loss=0.372, reward_mean=188.6, reward_threshold=222.0\n",
            " 101[ 55sec]: loss=0.371, reward_mean=186.6, reward_threshold=215.6\n",
            " 102[ 61sec]: loss=0.371, reward_mean=197.5, reward_threshold=246.3\n",
            " 103[ 57sec]: loss=0.372, reward_mean=192.3, reward_threshold=228.0\n",
            " 104[ 59sec]: loss=0.372, reward_mean=192.9, reward_threshold=225.0\n",
            " 105[ 57sec]: loss=0.371, reward_mean=190.4, reward_threshold=223.0\n",
            " 106[ 60sec]: loss=0.371, reward_mean=199.3, reward_threshold=241.0\n",
            " 107[ 59sec]: loss=0.370, reward_mean=197.9, reward_threshold=227.3\n",
            " 108[ 63sec]: loss=0.371, reward_mean=204.2, reward_threshold=256.6\n",
            "expected reward mean has been reached\n",
            "Total time taken: 3774.7853453159332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk1FZF7deZjf"
      },
      "source": [
        "# finally, close gym environment\n",
        "env_wrapper.close()\n",
        "env.close()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML8MwmC4eWEW"
      },
      "source": [
        "# save the model parameters\n",
        "torch.save(net, 'CartPole_simple.pth')"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}